# 执行计划：同步文件处理进度到前端

## 背景 (Background)
当前应用允许用户上传文件，但文件在后端的处理过程（如解析、分块、向量化）对前端是黑盒。用户在提交文件后只能等待，无法看到实时的处理进展，用户体验不佳。
与此同时，应用中的 "deep research" 功能已经通过 LangGraph 的流式事件（streaming events）实现了后端节点进度的实时展示。
本次任务的目标是借鉴 "deep research" 的实现方式，将文件处理过程也纳入 LangGraph 的流式框架中，从而在前端实时展示文件处理的各个阶段，提升用户体验。

## 目标 (Goal)
- **后端**: 将同步的文件处理逻辑改造为 LangGraph 中的异步节点。
- **后端**: 新的图节点在执行时，必须通过流向前端发送命名事件以报告进度。
- **前端**: 能够接收并解析来自后端的新的文件处理事件。
- **前端**: 在活动时间线（Activity Timeline）中清晰地展示文件处理的每个步骤。
- **验收标准**: 用户上传文件后，UI 界面上会像 "deep research" 一样，实时显示 "正在解析文档"、"正在生成向量" 等进度信息。
- **完成时间**: 2025-06-20

## 当前项目环境 (Current Project Environment)
- **技术栈**:
  - 前端: React, TypeScript, Vite, TailwindCSS, `@langchain/langgraph-sdk`
  - 后端: Python, FastAPI, LangChain, LangGraph
- **目录结构**:
  ```
  .
  ├── backend/src/agent/
  │   ├── app.py         # FastAPI 应用
  │   ├── graph.py       # LangGraph 定义
  │   └── tools_and_schemas.py # 工具和节点函数
  └── frontend/src/
      └── App.tsx        # 前端核心逻辑和 UI
  ```
- **当前配置**:
  - 前端使用 `useStream` hook 与后端 LangGraph 建立流式连接。
  - 文件上传通过独立的 `POST /uploadfile/` 接口同步处理。
  - 文件处理逻辑（解析、向量化等）目前与 `/uploadfile/` 接口耦合，未在 LangGraph 图中。

## 实施步骤 (Implementation Steps)

[x] AI: **步骤 1 - 后端：创建文件处理节点**
**任务描述**: 在 `backend/src/agent/tools_and_schemas.py` (或新建一个专门的文件) 中，创建新的 Python 函数作为 LangGraph 的节点。这些函数将负责文件处理的核心逻辑。
**预期结果**: 创建至少两个新函数：`load_and_split_document` 和 `generate_embeddings`。
**验证方法**: 函数可以被单独调用并成功执行其任务（如接收文件路径，返回文本块或向量）。
**文件路径**: [`backend/src/agent/tools_and_schemas.py`](backend/src/agent/tools_and_schemas.py)
**伪代码/命令**:
```python
# In tools_and_schemas.py

def load_and_split_document(state: AgentState) -> dict:
    """加载并分割文档"""
    filepath = state['filepath']
    # ... 使用 UnstructuredFileLoader 或类似工具加载文件
    # ... 使用 RecursiveCharacterTextSplitter 分割文本
    # ... 更新 state['documents']
    return {"documents": documents}

def generate_embeddings(state: AgentState) -> dict:
    """为文档块生成向量嵌入"""
    documents = state['documents']
    # ... 初始化 embedding model
    # ... 遍历 documents 并生成向量
    # ... 将向量存入 VectorStore
    # ... 更新 state['retriever']
    return {"retriever": retriever}
```
**执行说明**: 这些函数应遵循 LangGraph 节点的设计模式，接收 `state` 对象作为输入，并返回一个包含更新后状态的字典。

[x] AI: **步骤 2 - 后端：将新节点整合进 LangGraph**
**任务描述**: 在 `backend/src/agent/graph.py` 中，将上一步创建的 `load_and_split_document` 和 `generate_embeddings` 节点添加到 LangGraph 图中。需要定义这些节点之间的连接关系以及条件边（conditional edges），确保它们在提供了 `filepath` 时被触发。
**预期结果**: LangGraph 图现在包含文件处理的路径。
**验证方法**: 运行修改后的图，当输入包含 `filepath` 时，能够观察到文件处理节点被依次调用。
**文件路径**: [`backend/src/agent/graph.py`](backend/src/agent/graph.py)
**伪代码/命令**:
```python
# In graph.py

# ... 引用新创建的节点函数
from .tools_and_schemas import load_and_split_document, generate_embeddings

# ... 在图中添加新节点
workflow.add_node("file_processor", load_and_split_document)
workflow.add_node("embedder", generate_embeddings)

# ... 定义条件入口点
def route_initial(state):
    if state.get("filepath"):
        return "file_processor"
    else:
        return "planner" # or existing starting node

workflow.set_conditional_entry_point(route_initial)

# ... 定义新节点之间的边
workflow.add_edge("file_processor", "embedder")
workflow.add_edge("embedder", "planner") # or next logical step
```
**执行说明**: 关键在于设置一个条件入口点，根据输入中是否存在 `filepath` 来决定是先处理文件还是直接开始研究任务。

[x] AI: **步骤 3 - 后端：从新节点发送流式事件**
**任务描述**: 修改 `load_and_split_document` 和 `generate_embeddings` 节点（或其调用的工具），使其在执行时能通过 LangGraph 的流式机制向前端发送命名事件。
**预期结果**: 当图运行时，前端能够接收到如 `{"event": "file_processing", "data": ...}` 和 `{"event": "embedding", "data": ...}` 的流式消息。
**验证方法**: 使用 LangGraph 的 `stream` 方法调用图，并打印输出的每个事件，确认包含自定义的事件名称。
**文件路径**: [`backend/src/agent/tools_and_schemas.py`](backend/src/agent/tools_and_schemas.py)
**执行说明**: LangGraph 节点本身不直接发送事件，事件是在 `app.py` 中配置流时，根据节点名称自动生成的。因此，这一步主要是确保节点命名清晰（如 `file_processor`, `embedder`），前端将根据这些节点名来接收事件。

[x] AI: **步骤 4 - 前端：处理新的流式事件**
**任务描述**: 在 `frontend/src/App.tsx` 的 `onUpdateEvent` 回调函数中，添加新的 `else if` 逻辑来捕获和处理来自后端的 `file_processor` 和 `embedder` 事件。
**预期结果**: 当接收到新的事件时，能够正确解析并创建 `ProcessedEvent` 对象。
**验证方法**: 在 `onUpdateEvent` 中添加 `console.log(event)`，在触发文件上传后，观察浏览器控制台是否打印出后端发送的新事件。
**文件路径**: [`frontend/src/App.tsx`](frontend/src/App.tsx)
**伪代码/命令**:
```typescript
// In App.tsx, inside onUpdateEvent callback

// ... existing else if blocks

} else if (typeof event === 'object' && event !== null && 'file_processor' in event) {
  processedEvent = {
    title: "Processing Document",
    data: "Loading and splitting the uploaded file.",
  };
} else if (typeof event === 'object' && event !== null && 'embedder' in event) {
  processedEvent = {
    title: "Generating Embeddings",
    data: "Creating vector embeddings for document chunks.",
  };
}

// ... existing logic to setProcessedEventsTimeline
```
**执行说明**: 事件的 `key` 应该与 `graph.py` 中定义的节点名称（`file_processor`, `embedder`）完全匹配。

[x] AI: **步骤 5 - 后端：轻量化文件上传接口**
**任务描述**: 修改 `backend/src/agent/app.py` 中的 `/uploadfile/` 接口，移除所有耗时的文件处理逻辑。该接口现在只负责接收文件、保存到临时目录，并立即返回文件路径。
**预期结果**: `/uploadfile/` 接口响应时间极大缩短。
**验证方法**: 使用 Postman 或 curl 直接调用 `/uploadfile/` 接口，确认其响应速度非常快，并且文件被正确保存。
**文件路径**: [`backend/src/agent/app.py`](backend/src/agent/app.py)
**伪代码/命令**:
```python
# In app.py

@app.post("/uploadfile/")
async def upload_file(file: UploadFile = File(...)):
    # 移除所有解析、分块、向量化逻辑
    
    # 只保留保存文件的逻辑
    temp_dir = "temp_uploads"
    os.makedirs(temp_dir, exist_ok=True)
    filepath = os.path.join(temp_dir, file.filename)
    
    with open(filepath, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
        
    return {"filepath": filepath}
```
**执行说明**: 这是一个重要的优化，确保了前端在点击上传后能迅速得到响应，并将处理的等待过程完全交给流式UI来展示。

## 涉及的文件 (Involved Files)

| 文件路径 | 操作类型 | 用途说明 | 负责方 |
|---|---|---|---|
| [`backend/src/agent/graph.py`](backend/src/agent/graph.py) | 修改 | 添加文件处理节点和路由逻辑 | AI |
| [`backend/src/agent/tools_and_schemas.py`](backend/src/agent/tools_and_schemas.py) | 修改 | 创建文件处理节点的具体实现 | AI |
| [`backend/src/agent/app.py`](backend/src/agent/app.py) | 修改 | 简化 `/uploadfile/` 接口，使其只保存文件 | AI |
| [`frontend/src/App.tsx`](frontend/src/App.tsx) | 修改 | 在 `onUpdateEvent` 中增加对新事件的处理逻辑 | AI |

## 风险评估与应对策略 (Risk Assessment and Mitigation)

| 风险描述 | 影响程度 | 应对策略 | 负责方 |
|---|---|---|---|
| 后端图逻辑变得复杂，难以维护 | 中 | 保持节点功能单一、模块化。为新节点添加单元测试。在代码中添加清晰的注释说明数据流。 | AI |
| 大文件处理导致节点执行超时 | 高 | 1. 在前端限制上传文件的大小。 2. 后端节点设计为可处理大数据量，或进一步将大任务分解为更小的流式步骤（例如，分块处理并流式返回进度）。 | AI |
| 前后端事件名称不匹配导致UI不更新 | 低 | 确保前后端使用统一的、常量化的事件名称。在开发过程中通过 `console.log` 密切监视事件流。 | AI |

## 回滚计划 (Rollback Plan)
如果计划执行失败或引入了严重问题，将使用 Git 进行回滚。
1.  在开始修改前，确保所有代码已提交到版本控制。
2.  创建一个新的 feature 分支进行开发，如 `feature/file-progress-stream`。
3.  如果需要回滚，可以直接丢弃该分支，或使用 `git revert <commit_hash>` 来撤销相关的提交。
4.  具体命令：`git checkout main && git branch -D feature/file-progress-stream` 或 `git revert [commit1] [commit2] ...`。